{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2d0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d913fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 16개의 배열로 준비\n",
    "    # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "        \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "        \n",
    "        return  self.reward, self.done\n",
    " \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "    \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self,player):\n",
    "        # 0 1 2 3\n",
    "        # 4 5 6 7\n",
    "        # 8 9 10 11\n",
    "        # 12 13 14 15\n",
    "        # 승패 조건은 가로, 세로, 대각선 이 -1 이나 1 로 동일할 때 \n",
    "        end_condition = ((0,1,2,3),(4,5,6,7),(8,9,10,11),(12,13,14,15),(0,5,10,15),(3,6,9,12), (0,4,8,12), (1,5,9,13), (2,6,10,14), (3,7,11,15))\n",
    "        for line in end_condition:\n",
    "            if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                and self.board_a[line[2]] == self.board_a[line[3]] \\\n",
    "                and self.board_a[line[0]] != 0:\n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "        \n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self):\n",
    "        print(\"+----+----+----+----+\")\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if self.board_a[4*i+j] == 1:\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[4*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36471bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 상태 번호 표시\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  0 +  1 +  2 +  3 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  4 +  5 +  6 +  7 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  8 +  9 + 10 + 11 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "                        \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0764bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    # 확률에 음수가 있을 경우 전부 양수가 되도록 보정\n",
    "    def positive_normalize(self, policy):\n",
    "        min_val = min(policy.values())\n",
    "        if min_val < 0:\n",
    "            for action in policy:\n",
    "                policy[action] -= min_val\n",
    "\n",
    "        total_sum = sum(policy.values())\n",
    "\n",
    "        if total_sum > 0:\n",
    "            for action in policy:\n",
    "                policy[action] /= total_sum\n",
    "        else:\n",
    "            for action in policy:\n",
    "                policy[action] = 1.0 / len(policy)\n",
    "\n",
    "    # 정책 기반 행동 선택\n",
    "    def select_action(self, state, policy, available_actions):\n",
    "        # 현재 상태의 정책이 없으면 균등 확률로 초기화\n",
    "        if state not in policy:\n",
    "            policy[state] = {}\n",
    "            probability = 1.0 / len(available_actions)\n",
    "            for action in available_actions:\n",
    "                policy[state][action] = probability\n",
    "\n",
    "        # Gibbs 소프트맥스 함수로 선택될 확률을 조정\n",
    "        exp_values = {}\n",
    "        for action in available_actions:\n",
    "            if action in policy[state]:\n",
    "                value = policy[state][action]\n",
    "            else:\n",
    "                value = 0.0\n",
    "            exp_values[action] = np.exp(value)\n",
    "\n",
    "        # 확률 계산\n",
    "        total_exp_value = sum(exp_values.values())\n",
    "        probabilities = {}\n",
    "        if total_exp_value > 0:\n",
    "            for action in available_actions:\n",
    "                probabilities[action] = exp_values[action] / total_exp_value\n",
    "        else:\n",
    "            probability = 1.0 / len(available_actions)\n",
    "            for action in available_actions:\n",
    "                probabilities[action] = probability\n",
    "\n",
    "        # 행동 선택\n",
    "        actions = list(available_actions)\n",
    "        pr = []\n",
    "        for action in actions:\n",
    "            pr.append(probabilities[action])\n",
    "\n",
    "        i = np.random.choice(range(len(actions)), p=pr)\n",
    "        return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec15630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic:\n",
    "    is_trained = False\n",
    "    trained_policy = {}\n",
    "    trained_optimal_policy = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Actor Critic\"\n",
    "        self.agent = Agent()\n",
    "        \n",
    "        # 학습 여부 확인\n",
    "        if Actor_Critic.is_trained:\n",
    "            self.policy = Actor_Critic.trained_policy\n",
    "            self.optimal_policy = Actor_Critic.trained_optimal_policy\n",
    "            return\n",
    "        \n",
    "        # 액터-크리틱 알고리즘 초기화\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # 환경 초기화\n",
    "        self.env = Environment()\n",
    "        \n",
    "        # 학습 파라미터 설정\n",
    "        self.gamma = 0.9 \n",
    "        self.alpha = 0.2  \n",
    "        self.beta = 0.15  \n",
    "        \n",
    "        self.V = {}      \n",
    "        self.policy = {}  \n",
    "        self.optimal_policy = {}  \n",
    "        \n",
    "        self.max_episode = 14000\n",
    "        self.max_step = 16      \n",
    "        \n",
    "        # 각 에피소드에 대해 반복:\n",
    "        for epi in tqdm(range(self.max_episode)):\n",
    "            # 환경 초기화\n",
    "            self.env = Environment()\n",
    "            \n",
    "            # 에피소드의 각 스텝에 대해 반복:\n",
    "            for k in range(self.max_step):\n",
    "                current_state = tuple(self.env.board_a)\n",
    "                \n",
    "                # 행동 선택\n",
    "                action = self.select_action(self.env, 1)\n",
    "                if action is None:\n",
    "                    break\n",
    "                    \n",
    "                # 환경에 행동 적용\n",
    "                self.env.board_a[action] = 1\n",
    "                \n",
    "                # 보상과 종료 여부 확인\n",
    "                self.env.end_check(1)\n",
    "                reward = self.env.reward\n",
    "                done = self.env.done\n",
    "                \n",
    "                # 다음 상태 s'\n",
    "                next_state = tuple(self.env.board_a)\n",
    "                \n",
    "                # 새로운 상태 가치 함수 초기화\n",
    "                if current_state not in self.V:\n",
    "                    self.V[current_state] = 0.0\n",
    "                if next_state not in self.V:\n",
    "                    self.V[next_state] = 0.0\n",
    "                \n",
    "                # 크리틱 학습: 시간차 에러 계산\n",
    "                td_error = reward + self.gamma * self.V[next_state] - self.V[current_state]\n",
    "                \n",
    "                # 상태 가치 함수 업데이트\n",
    "                self.V[current_state] += self.alpha * td_error\n",
    "                \n",
    "                # 액터 정책 초기화 (필요한 경우)\n",
    "                if current_state not in self.policy:\n",
    "                    self.policy[current_state] = {}\n",
    "                \n",
    "                # 액터 학습: 행동 가치 업데이트\n",
    "                if action not in self.policy[current_state]:\n",
    "                    self.policy[current_state][action] = 0.0\n",
    "                    \n",
    "                self.policy[current_state][action] += td_error * self.beta\n",
    "                \n",
    "                # 확률에 음수가 있을 경우 전부 양수가 되도록 보정\n",
    "                self.agent.positive_normalize(self.policy[current_state])\n",
    "                \n",
    "                # s가 마지막 상태라면 종료\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # 학습된 정책에서 최적 행동을 추출\n",
    "            for state in self.policy:\n",
    "                if self.policy[state]:\n",
    "                    self.optimal_policy[state] = max(self.policy[state], key=self.policy[state].get)\n",
    "        \n",
    "        # 학습 완료 후 학습된 정책 저장\n",
    "        Actor_Critic.trained_policy = self.policy\n",
    "        Actor_Critic.trained_optimal_policy = self.optimal_policy\n",
    "        Actor_Critic.is_trained = True\n",
    "    \n",
    "    def select_action(self, env, player):\n",
    "        state = tuple(env.board_a)\n",
    "        available_actions = env.get_action()\n",
    "        if not available_actions:\n",
    "            return None\n",
    "        return self.agent.select_action(state, self.policy, available_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b1c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                           | 270/14500 [00:01<01:27, 162.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19412\\589468479.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#p1 = Human_player()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActor_Critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHuman_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19412\\2453645956.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# 행동 선택\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19412\\2453645956.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, env, player)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19412\\2501134907.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state, policy, available_actions)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#p1 = Human_player()\n",
    "p1 = Actor_Critic()\n",
    "\n",
    "p2 = Human_player()\n",
    "#p2 = Actor_Critic()\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d2bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
