{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcd42ae",
   "metadata": {},
   "source": [
    "사용 라이브러리 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2306ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a73a7e",
   "metadata": {},
   "source": [
    "JSON 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d53942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#JSON 파일 경로\n",
    "json_path = \"C:\\\\Users\\\\USER\\\\Downloads\\\\tictactoe_data_complete.json\" # 파일경로\n",
    "\n",
    "#JSON 불러오기\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 내부 데이터 추출\n",
    "game_data = data[\"game_data\"]\n",
    "\n",
    "# 데이터 순서 랜덤화\n",
    "np.random.shuffle(game_data)\n",
    "\n",
    "# 훈련 데이터 80% / 검증 데이터 20% 분리\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(game_data) * split_ratio)\n",
    "\n",
    "train_data = game_data[:split_index]\n",
    "test_data = game_data[split_index:]\n",
    "\n",
    "# board_state 및 action 추출 함수 정의\n",
    "def extract_state_action(data):\n",
    "    board_state = []\n",
    "    action = []\n",
    "    for item in data:\n",
    "        board_state.append(item[\"board_state_before\"])  \n",
    "        action.append(item[\"action\"])             \n",
    "    return np.array(board_state), np.array(action)\n",
    "\n",
    "# 변환\n",
    "input_shape = (4, 4, 1)\n",
    "num_classes = 16\n",
    "\n",
    "x_train, y_train = extract_state_action(train_data)\n",
    "x_test, y_test = extract_state_action(test_data)\n",
    "\n",
    "x_train = x_train.reshape(-1, 4, 4, 1)\n",
    "x_test = x_test.reshape(-1, 4, 4, 1)\n",
    "\n",
    "# y 데이터 원-핫 인코딩\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# #특정 컬럼 10개씩 출력하는 함수\n",
    "# def print_column_samples(field_name, num_samples=10):\n",
    "#     print(f\"\\n--- {field_name} (Top {num_samples}) ---\")\n",
    "#     for i in range(min(num_samples, len(game_data))):\n",
    "#         print(f\"[{i}] {game_data[i][field_name]}\")\n",
    "\n",
    "# #출력\n",
    "# print_column_samples(\"board_state_before\")\n",
    "# print_column_samples(\"board_state_current\")\n",
    "# print_column_samples(\"action\")\n",
    "# print_column_samples(\"player\")\n",
    "# print_column_samples(\"winner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7a8d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 16개의 배열로 준비\n",
    "    # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "        \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "        \n",
    "        return  self.reward, self.done\n",
    " \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "    \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self,player):\n",
    "        # 0 1 2 3\n",
    "        # 4 5 6 7\n",
    "        # 8 9 10 11\n",
    "        # 12 13 14 15\n",
    "        # 승패 조건은 가로, 세로, 대각선 이 -1 이나 1 로 동일할 때 \n",
    "        end_condition = ((0,1,2,3),(4,5,6,7),(8,9,10,11),(12,13,14,15),(0,5,10,15),(3,6,9,12), (0,4,8,12), (1,5,9,13), (2,6,10,14), (3,7,11,15))\n",
    "        for line in end_condition:\n",
    "            if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                and self.board_a[line[2]] == self.board_a[line[3]] \\\n",
    "                and self.board_a[line[0]] != 0:\n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "        \n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self):\n",
    "        print(\"+----+----+----+----+\")\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if self.board_a[4*i+j] == 1:\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[4*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2189907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 상태 번호 표시\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  0 +  1 +  2 +  3 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  4 +  5 +  6 +  7 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  8 +  9 + 10 + 11 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "                        \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60744c",
   "metadata": {},
   "source": [
    "합성곱신경망과 완전연결계층 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66785ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras import backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "\n",
    "# 합성곱층 추가\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape, name='Conv2D_layer'))\n",
    "\n",
    "# 최대 풀링층 추가\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name = 'MaxPooling2D_layer'))\n",
    "\n",
    "# 플래튼층 추가\n",
    "model.add(Flatten(name='Flatten_layer'))\n",
    "\n",
    "# 드롭아웃 적용\n",
    "#model.add(Dropout(0.3)) \n",
    "\n",
    "# 완전연결계층 추가\n",
    "model.add(Dense(32, activation='relu', name='Dense1_layer'))\n",
    "model.add(Dense(num_classes, activation='softmax', name='Dense2_layer'))\n",
    "\n",
    "# 학습 방법 추가\n",
    "model.compile(loss = keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "# 모델 확인\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aef20ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10004 samples, validate on 2502 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 2.7594 - acc: 0.1002 - val_loss: 2.7361 - val_acc: 0.1211\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.7202 - acc: 0.1186 - val_loss: 2.6948 - val_acc: 0.1287\n",
      "Epoch 3/100\n",
      " - 1s - loss: 2.6947 - acc: 0.1261 - val_loss: 2.6794 - val_acc: 0.1279\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.6804 - acc: 0.1338 - val_loss: 2.6697 - val_acc: 0.1315\n",
      "Epoch 5/100\n",
      " - 1s - loss: 2.6672 - acc: 0.1429 - val_loss: 2.6591 - val_acc: 0.1427\n",
      "Epoch 6/100\n",
      " - 1s - loss: 2.6572 - acc: 0.1434 - val_loss: 2.6606 - val_acc: 0.1419\n",
      "Epoch 7/100\n",
      " - 1s - loss: 2.6486 - acc: 0.1451 - val_loss: 2.6480 - val_acc: 0.1419\n",
      "Epoch 8/100\n",
      " - 1s - loss: 2.6405 - acc: 0.1514 - val_loss: 2.6451 - val_acc: 0.1435\n",
      "Epoch 9/100\n",
      " - 1s - loss: 2.6325 - acc: 0.1516 - val_loss: 2.6469 - val_acc: 0.1439\n",
      "Epoch 10/100\n",
      " - 1s - loss: 2.6261 - acc: 0.1556 - val_loss: 2.6367 - val_acc: 0.1443\n",
      "Epoch 11/100\n",
      " - 1s - loss: 2.6201 - acc: 0.1530 - val_loss: 2.6380 - val_acc: 0.1543\n",
      "Epoch 12/100\n",
      " - 1s - loss: 2.6152 - acc: 0.1587 - val_loss: 2.6343 - val_acc: 0.1519\n",
      "Epoch 13/100\n",
      " - 1s - loss: 2.6109 - acc: 0.1622 - val_loss: 2.6355 - val_acc: 0.1531\n",
      "Epoch 14/100\n",
      " - 1s - loss: 2.6064 - acc: 0.1603 - val_loss: 2.6343 - val_acc: 0.1507\n",
      "Epoch 15/100\n",
      " - 1s - loss: 2.6023 - acc: 0.1643 - val_loss: 2.6323 - val_acc: 0.1567\n",
      "Epoch 16/100\n",
      " - 1s - loss: 2.5981 - acc: 0.1677 - val_loss: 2.6316 - val_acc: 0.1535\n",
      "Epoch 17/100\n",
      " - 1s - loss: 2.5929 - acc: 0.1644 - val_loss: 2.6313 - val_acc: 0.1539\n",
      "Epoch 18/100\n",
      " - 1s - loss: 2.5916 - acc: 0.1662 - val_loss: 2.6261 - val_acc: 0.1579\n",
      "Epoch 19/100\n",
      " - 1s - loss: 2.5868 - acc: 0.1649 - val_loss: 2.6232 - val_acc: 0.1567\n",
      "Epoch 20/100\n",
      " - 1s - loss: 2.5835 - acc: 0.1673 - val_loss: 2.6309 - val_acc: 0.1547\n",
      "Epoch 21/100\n",
      " - 1s - loss: 2.5804 - acc: 0.1684 - val_loss: 2.6310 - val_acc: 0.1499\n",
      "Epoch 22/100\n",
      " - 1s - loss: 2.5780 - acc: 0.1699 - val_loss: 2.6217 - val_acc: 0.1563\n",
      "Epoch 23/100\n",
      " - 1s - loss: 2.5737 - acc: 0.1682 - val_loss: 2.6188 - val_acc: 0.1571\n",
      "Epoch 24/100\n",
      " - 1s - loss: 2.5708 - acc: 0.1716 - val_loss: 2.6226 - val_acc: 0.1511\n",
      "Epoch 25/100\n",
      " - 1s - loss: 2.5673 - acc: 0.1716 - val_loss: 2.6196 - val_acc: 0.1523\n",
      "Epoch 26/100\n",
      " - 1s - loss: 2.5642 - acc: 0.1740 - val_loss: 2.6143 - val_acc: 0.1531\n",
      "Epoch 27/100\n",
      " - 1s - loss: 2.5604 - acc: 0.1754 - val_loss: 2.6224 - val_acc: 0.1491\n",
      "Epoch 28/100\n",
      " - 1s - loss: 2.5589 - acc: 0.1750 - val_loss: 2.6174 - val_acc: 0.1583\n",
      "Epoch 29/100\n",
      " - 1s - loss: 2.5545 - acc: 0.1811 - val_loss: 2.6203 - val_acc: 0.1499\n",
      "Epoch 30/100\n",
      " - 1s - loss: 2.5525 - acc: 0.1798 - val_loss: 2.6189 - val_acc: 0.1539\n",
      "Epoch 31/100\n",
      " - 1s - loss: 2.5495 - acc: 0.1786 - val_loss: 2.6159 - val_acc: 0.1535\n",
      "Epoch 32/100\n",
      " - 1s - loss: 2.5464 - acc: 0.1813 - val_loss: 2.6138 - val_acc: 0.1551\n",
      "Epoch 33/100\n",
      " - 1s - loss: 2.5439 - acc: 0.1840 - val_loss: 2.6201 - val_acc: 0.1595\n",
      "Epoch 34/100\n",
      " - 1s - loss: 2.5404 - acc: 0.1845 - val_loss: 2.6243 - val_acc: 0.1539\n",
      "Epoch 35/100\n",
      " - 1s - loss: 2.5388 - acc: 0.1833 - val_loss: 2.6167 - val_acc: 0.1547\n",
      "Epoch 36/100\n",
      " - 1s - loss: 2.5361 - acc: 0.1851 - val_loss: 2.6267 - val_acc: 0.1559\n",
      "Epoch 37/100\n",
      " - 1s - loss: 2.5342 - acc: 0.1898 - val_loss: 2.6205 - val_acc: 0.1591\n",
      "Epoch 38/100\n",
      " - 1s - loss: 2.5314 - acc: 0.1854 - val_loss: 2.6282 - val_acc: 0.1643\n",
      "Epoch 39/100\n",
      " - 1s - loss: 2.5293 - acc: 0.1869 - val_loss: 2.6228 - val_acc: 0.1591\n",
      "Epoch 40/100\n",
      " - 1s - loss: 2.5265 - acc: 0.1893 - val_loss: 2.6242 - val_acc: 0.1547\n",
      "Epoch 41/100\n",
      " - 1s - loss: 2.5264 - acc: 0.1886 - val_loss: 2.6182 - val_acc: 0.1571\n",
      "Epoch 42/100\n",
      " - 1s - loss: 2.5233 - acc: 0.1930 - val_loss: 2.6228 - val_acc: 0.1623\n",
      "Epoch 43/100\n",
      " - 1s - loss: 2.5209 - acc: 0.1916 - val_loss: 2.6250 - val_acc: 0.1559\n",
      "Epoch 44/100\n",
      " - 1s - loss: 2.5198 - acc: 0.1901 - val_loss: 2.6346 - val_acc: 0.1639\n",
      "Epoch 45/100\n",
      " - 1s - loss: 2.5184 - acc: 0.1927 - val_loss: 2.6268 - val_acc: 0.1619\n",
      "Epoch 46/100\n",
      " - 1s - loss: 2.5166 - acc: 0.1936 - val_loss: 2.6278 - val_acc: 0.1579\n",
      "Epoch 47/100\n",
      " - 1s - loss: 2.5146 - acc: 0.1944 - val_loss: 2.6302 - val_acc: 0.1627\n",
      "Epoch 48/100\n",
      " - 1s - loss: 2.5131 - acc: 0.1917 - val_loss: 2.6220 - val_acc: 0.1583\n",
      "Epoch 49/100\n",
      " - 1s - loss: 2.5120 - acc: 0.1944 - val_loss: 2.6272 - val_acc: 0.1567\n",
      "Epoch 50/100\n",
      " - 1s - loss: 2.5103 - acc: 0.1960 - val_loss: 2.6237 - val_acc: 0.1607\n",
      "Epoch 51/100\n",
      " - 1s - loss: 2.5094 - acc: 0.1929 - val_loss: 2.6258 - val_acc: 0.1611\n",
      "Epoch 52/100\n",
      " - 1s - loss: 2.5080 - acc: 0.1965 - val_loss: 2.6261 - val_acc: 0.1635\n",
      "Epoch 53/100\n",
      " - 1s - loss: 2.5065 - acc: 0.1947 - val_loss: 2.6276 - val_acc: 0.1595\n",
      "Epoch 54/100\n",
      " - 1s - loss: 2.5047 - acc: 0.1950 - val_loss: 2.6317 - val_acc: 0.1647\n",
      "Epoch 55/100\n",
      " - 1s - loss: 2.5033 - acc: 0.1959 - val_loss: 2.6325 - val_acc: 0.1663\n",
      "Epoch 56/100\n",
      " - 1s - loss: 2.5016 - acc: 0.1962 - val_loss: 2.6321 - val_acc: 0.1651\n",
      "Epoch 57/100\n",
      " - 1s - loss: 2.5003 - acc: 0.1945 - val_loss: 2.6309 - val_acc: 0.1619\n",
      "Epoch 58/100\n",
      " - 1s - loss: 2.4996 - acc: 0.1968 - val_loss: 2.6316 - val_acc: 0.1599\n",
      "Epoch 59/100\n",
      " - 1s - loss: 2.4984 - acc: 0.1950 - val_loss: 2.6287 - val_acc: 0.1587\n",
      "Epoch 60/100\n",
      " - 1s - loss: 2.4973 - acc: 0.1996 - val_loss: 2.6355 - val_acc: 0.1639\n",
      "Epoch 61/100\n",
      " - 1s - loss: 2.4959 - acc: 0.1991 - val_loss: 2.6301 - val_acc: 0.1659\n",
      "Epoch 62/100\n",
      " - 1s - loss: 2.4953 - acc: 0.1956 - val_loss: 2.6272 - val_acc: 0.1603\n",
      "Epoch 63/100\n",
      " - 1s - loss: 2.4937 - acc: 0.1976 - val_loss: 2.6244 - val_acc: 0.1599\n",
      "Epoch 64/100\n",
      " - 1s - loss: 2.4912 - acc: 0.1993 - val_loss: 2.6327 - val_acc: 0.1615\n",
      "Epoch 65/100\n",
      " - 1s - loss: 2.4918 - acc: 0.1999 - val_loss: 2.6279 - val_acc: 0.1651\n",
      "Epoch 66/100\n",
      " - 1s - loss: 2.4898 - acc: 0.2000 - val_loss: 2.6310 - val_acc: 0.1651\n",
      "Epoch 67/100\n",
      " - 1s - loss: 2.4881 - acc: 0.1986 - val_loss: 2.6300 - val_acc: 0.1639\n",
      "Epoch 68/100\n",
      " - 1s - loss: 2.4876 - acc: 0.2000 - val_loss: 2.6250 - val_acc: 0.1647\n",
      "Epoch 69/100\n",
      " - 1s - loss: 2.4866 - acc: 0.1997 - val_loss: 2.6405 - val_acc: 0.1635\n",
      "Epoch 70/100\n",
      " - 1s - loss: 2.4856 - acc: 0.1987 - val_loss: 2.6304 - val_acc: 0.1635\n",
      "Epoch 71/100\n",
      " - 1s - loss: 2.4833 - acc: 0.2051 - val_loss: 2.6389 - val_acc: 0.1591\n",
      "Epoch 72/100\n",
      " - 1s - loss: 2.4834 - acc: 0.2027 - val_loss: 2.6333 - val_acc: 0.1659\n",
      "Epoch 73/100\n",
      " - 1s - loss: 2.4814 - acc: 0.2064 - val_loss: 2.6378 - val_acc: 0.1659\n",
      "Epoch 74/100\n",
      " - 1s - loss: 2.4813 - acc: 0.2021 - val_loss: 2.6339 - val_acc: 0.1687\n",
      "Epoch 75/100\n",
      " - 1s - loss: 2.4789 - acc: 0.2046 - val_loss: 2.6338 - val_acc: 0.1639\n",
      "Epoch 76/100\n",
      " - 1s - loss: 2.4790 - acc: 0.2015 - val_loss: 2.6323 - val_acc: 0.1699\n",
      "Epoch 77/100\n",
      " - 1s - loss: 2.4769 - acc: 0.2022 - val_loss: 2.6368 - val_acc: 0.1675\n",
      "Epoch 78/100\n",
      " - 1s - loss: 2.4770 - acc: 0.2041 - val_loss: 2.6433 - val_acc: 0.1635\n",
      "Epoch 79/100\n",
      " - 1s - loss: 2.4762 - acc: 0.2044 - val_loss: 2.6343 - val_acc: 0.1627\n",
      "Epoch 80/100\n",
      " - 1s - loss: 2.4756 - acc: 0.2059 - val_loss: 2.6389 - val_acc: 0.1655\n",
      "Epoch 81/100\n",
      " - 1s - loss: 2.4741 - acc: 0.2026 - val_loss: 2.6363 - val_acc: 0.1643\n",
      "Epoch 82/100\n",
      " - 1s - loss: 2.4729 - acc: 0.2073 - val_loss: 2.6351 - val_acc: 0.1655\n",
      "Epoch 83/100\n",
      " - 1s - loss: 2.4725 - acc: 0.2094 - val_loss: 2.6387 - val_acc: 0.1623\n",
      "Epoch 84/100\n",
      " - 1s - loss: 2.4716 - acc: 0.2067 - val_loss: 2.6350 - val_acc: 0.1651\n",
      "Epoch 85/100\n",
      " - 1s - loss: 2.4701 - acc: 0.2065 - val_loss: 2.6377 - val_acc: 0.1675\n",
      "Epoch 86/100\n",
      " - 1s - loss: 2.4692 - acc: 0.2060 - val_loss: 2.6386 - val_acc: 0.1663\n",
      "Epoch 87/100\n",
      " - 1s - loss: 2.4695 - acc: 0.2089 - val_loss: 2.6342 - val_acc: 0.1643\n",
      "Epoch 88/100\n",
      " - 1s - loss: 2.4674 - acc: 0.2090 - val_loss: 2.6439 - val_acc: 0.1655\n",
      "Epoch 89/100\n",
      " - 1s - loss: 2.4672 - acc: 0.2088 - val_loss: 2.6383 - val_acc: 0.1683\n",
      "Epoch 90/100\n",
      " - 1s - loss: 2.4668 - acc: 0.2109 - val_loss: 2.6451 - val_acc: 0.1771\n",
      "Epoch 91/100\n",
      " - 1s - loss: 2.4659 - acc: 0.2078 - val_loss: 2.6445 - val_acc: 0.1711\n",
      "Epoch 92/100\n",
      " - 1s - loss: 2.4657 - acc: 0.2097 - val_loss: 2.6356 - val_acc: 0.1703\n",
      "Epoch 93/100\n",
      " - 1s - loss: 2.4635 - acc: 0.2112 - val_loss: 2.6486 - val_acc: 0.1631\n",
      "Epoch 94/100\n",
      " - 1s - loss: 2.4640 - acc: 0.2083 - val_loss: 2.6435 - val_acc: 0.1675\n",
      "Epoch 95/100\n",
      " - 1s - loss: 2.4626 - acc: 0.2102 - val_loss: 2.6397 - val_acc: 0.1695\n",
      "Epoch 96/100\n",
      " - 1s - loss: 2.4614 - acc: 0.2137 - val_loss: 2.6417 - val_acc: 0.1731\n",
      "Epoch 97/100\n",
      " - 1s - loss: 2.4620 - acc: 0.2097 - val_loss: 2.6425 - val_acc: 0.1711\n",
      "Epoch 98/100\n",
      " - 1s - loss: 2.4615 - acc: 0.2111 - val_loss: 2.6422 - val_acc: 0.1715\n",
      "Epoch 99/100\n",
      " - 1s - loss: 2.4605 - acc: 0.2108 - val_loss: 2.6404 - val_acc: 0.1735\n",
      "Epoch 100/100\n",
      " - 1s - loss: 2.4606 - acc: 0.2120 - val_loss: 2.6453 - val_acc: 0.1691\n",
      "Test loss :  2.6452540444146146\n",
      "Test accuracy :  0.16906474821335024\n"
     ]
    }
   ],
   "source": [
    "# 배치 사이즈와 에폭 수 정의\n",
    "batch_size = 32 # 기존 128\n",
    "epochs = 100\n",
    "\n",
    "# 학습 전 합성곱신경망 저장\n",
    "weight_before, bias_before = model.layers[0].get_weights()\n",
    "\n",
    "# 합성곱신경망 학습\n",
    "hist = model.fit(x_train, y_train,\n",
    "                batch_size = batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=2,\n",
    "                validation_data=(x_test, y_test))\n",
    "\n",
    "# 학습 후 합성곱신경망 필터 저장\n",
    "weight_after, bias_after = model.layers[0].get_weights()\n",
    "\n",
    "# 학습된 합성공 신경망 성능 확인\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss : ', score[0])\n",
    "print('Test accuracy : ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5df16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_player():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.name = \"CNN player\"\n",
    "        self.model = model\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "\n",
    "        board_2d = env.board_a.reshape(4, 4)\n",
    "        \n",
    "        board_input = board_2d.reshape(1, 4, 4, 1)\n",
    "        \n",
    "        # 성능 검증\n",
    "        predictions = self.model.predict(board_input, verbose=0)\n",
    "        move_probabilities = predictions[0]\n",
    "        \n",
    "        # 가능한 행동 확인\n",
    "        available_actions = env.get_action()\n",
    "        \n",
    "        # 가능한 행동들 중에서 확률이 가장 높은 것 선택\n",
    "        best_action = None\n",
    "\n",
    "        sorted_actions = np.argsort(move_probabilities)[::-1]\n",
    "        \n",
    "        for action in sorted_actions:\n",
    "            if action in available_actions:\n",
    "                best_action = action\n",
    "                break\n",
    "        \n",
    "        if best_action is None:\n",
    "            best_action = np.random.choice(available_actions)\n",
    "            \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd8238",
   "metadata": {},
   "source": [
    "실제 대전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0689dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl player : CNN player\n",
      "p2 player : Human player\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "possible actions = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 12\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |    |    |    |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  O |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |    |    |    |\n",
      "+----+----+----+----+\n",
      "possible actions = [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 13\n",
      "+----+----+----+----+\n",
      "|  O |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  X |    |    |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  X |    |    |\n",
      "+----+----+----+----+\n",
      "possible actions = [2, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 14\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  X |  X |    |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  O |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  X |  X |    |\n",
      "+----+----+----+----+\n",
      "winner is p1(CNN player)\n",
      "final result\n",
      "+----+----+----+----+\n",
      "|  O |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  X |  X |    |\n",
      "+----+----+----+----+\n",
      "More Game? (y/n)n\n",
      "p1(CNN player) = 1 p2(Human player) = 0 draw = 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#p1 = Human_player()\n",
    "p1 = CNN_player(model)\n",
    "\n",
    "p2 = Human_player()\n",
    "#p2 = CNN_player(model)\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3590e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
